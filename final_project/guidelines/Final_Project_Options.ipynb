{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: BBC 21st Century's greatest films\n",
    "\n",
    "In 2016 the BBC polled 177 film critics to get their picks for the best films of the century so far. While the BBC's [aggregate poll](http://www.bbc.com/culture/story/20160819-the-21st-centurys-100-greatest-films) is interesting, the long list including everyone who voted is perhaps more revealing from the data standpoint:\n",
    "\n",
    "http://www.bbc.com/culture/story/20160819-the-21st-centurys-100-greatest-films-who-voted\n",
    "\n",
    "Our goal for this project would be to scrape this page -- using beautiful soup and regular expressions -- in order to make a searchable database that would allow us to investigate all of the films listed by all of the critics.\n",
    "\n",
    "If you choose this project, the main challenge will be to design a database schema that organizes information by film, director, and/or by critic. \n",
    "\n",
    "From a geocoding standpoint we would visualize this data set based on the country of the critics, and we could, with extra research, view the data set by the country of the filmmaker. Perhaps there is more we could do geographically...\n",
    "\n",
    "From an exploratory standpoint, here are some questions we could ask: \n",
    "\n",
    "1. Which countries have the most directors?\n",
    "2. Which directors have the most movies selected?\n",
    "3. What year had the most movies selected?\n",
    "4. What are some other questions you can think of?\n",
    "\n",
    "The overall challenge, once the initial page is properly formatted into a database, is to find a way to enhance this data set with another source, and integrate that information with the information from the BBC poll.\n",
    "\n",
    "The most obvious piece of missing data is the country of origin of the director. Country of origin for the movie it is actually quite difficult, productions are very international. One could also look into other secondary levels of information such as box office gross in different countries. It is up to you--the real challenge is twofold:\n",
    "\n",
    "1. Deciding on a secondary level of information that you think will be meaningful and interesting\n",
    "\n",
    "2. Finding, capturing, transforming  and joining that secondary data.\n",
    "\n",
    "**The Advantages of this option:** It is very focused. It is flexible enough for you to take in your own direction, it is contained enough that is likely to be completed by the deadline. (While mapping by country is the most obvious route--world maps are inherently flawed. Feel free to focus on a single country. But if you do: try to find second sources that bring us into greater depth. Also feel free to bring in other this from other sources especially those representing different voices from different countries.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Supreme Court Arguments\n",
    "Create a database of arguments before the US Supreme Court. The Supreme Court makes transcriptions of all arguments available on its website:\n",
    "\n",
    "https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx\n",
    "\n",
    "These arguments are available as PDFs. The downloading and conversion of PDFs to text is neither trivial nor particularly interesting of a challenge. The real challenge is what to do with the data once we have the text. \n",
    "\n",
    "Below, I show the code that I used to download all the PDFs of the the oral arguments before the Supreme Court from 2018, which I then convert to text files: that is where the fun starts. The goal of this project will be to use regular expressions to parse the text of the oral arguments, so that we can then search and measure the words spoken by the Supreme Court justices from case to case and across cases.\n",
    "\n",
    "The end goal is open: we want to have well structured data with the words of the justices entered. We also want to have a useful data set for each case that includes further information such as the final decision, the votes of each justice, who wrote the decision (and perhaps the dissenting opinion too). It is up to you what should ultimately be included and how we will be able to search through the data and view results.\n",
    "\n",
    "From an exploratory standpoint, here are some questions we could ask: \n",
    "\n",
    "1. Which justice speaks the most from case to case?\n",
    "2. What are the most frequent words used by each justice across cases?\n",
    "3. Does frequency of speech during arguments have any relationship to the decision? (hint: usually not)\n",
    "4. What are some other questions you can think of?\n",
    "\n",
    "Mapping: Supreme Court cases originate from district or appellate courts. The maps for this should be relatively simple: \n",
    "\n",
    "It should show the 13 Judicial districts as a way of locating the cases.  The map will serve simply as a basis for exploring cases--which districts produced the greatest number of cases. Although you can certainly try to come up with a more creative way of mapping the cases. Or you can choose not to map.\n",
    "\n",
    "The presentation of cases (headline and article) is a much more interpretive one, the recommended approach would be to come up with categories for the types of cases--what is that stake for each case.\n",
    "\n",
    "The real challenge will be building a database of Supreme Court transcripts, that includes the docket number, the original appellate court (and other docket information) along with the decision and the justices who voted. Once this database is built, the main question is building/finding/exploring categories of interpretation for the cases. \n",
    "\n",
    "Beyond the scraping and downloading PDFs (see below), other primary sources to scrape and integrate include:\n",
    "\n",
    "Geographical locations:\n",
    "https://system.uslegal.com/us-courts-of-appeals/\n",
    "\n",
    "Transcripts by year\n",
    "https://www.supremecourt.gov/oral_arguments/argument_transcript/2018\n",
    "\n",
    "Dockets buy circuit court:\n",
    "https://www.supremecourt.gov/orders/ordersbycircuit/ordercasebycircuit/061118OrderCasesByCircuit\n",
    "\n",
    "Dockett information by case:\n",
    "https://www.supremecourt.gov/search.aspx?filename=/docket/docketfiles/html/public/17-7919.html\n",
    "\n",
    "Opinions (as seen in Homework 3):\n",
    "https://www.supremecourt.gov/opinions/slipopinion/17\n",
    "\n",
    "**Advantages of this option:** scraping is minimal--regular expressions are the main tool involved. This central challenge is figuring out how to categorize and analyze a year worth of arguments before the Supreme Court. You will rely mainly on speaker, word frequency, specific word searches, and your own reading of some or all of the texts yourself. (The map is not a particularly useful framework for this project--therefore most of the important work will be conceptual and behind the scenes. This project will allow you to contend with the challenges text analysis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Federal Court Appointments\n",
    "Relatedly, one of the major news stories about the current presidency and Republican hold of both houses of congress, is the speed with which federal judicial appointments are being made. [This article in the Washington Post addresses the complexity of this issue.](https://www.washingtonpost.com/news/monkey-cage/wp/2018/06/14/are-trumps-judicial-nominees-really-being-confirmed-at-a-record-pace-the-answer-is-complicated/)\n",
    "\n",
    "The make up of the federal court system is relatively complex. Under the Supreme Court there are 13 appellate courts that represent 13 different districts. Below these appellate courts are 96 district courts. These are the pathways through which a case must travel to eventually reach the Supreme Court. It is the under power of the president and congress to nominate and approve judges for these courts.\n",
    "\n",
    "The goal of this project is to create an integrated database of judgeships and vacancies in the 13 appellate and/or 96 District courts. The map will serve as an exploratory interface for understanding the composition of these counts, and/or the current or historical transformation the federal court system. Via this page is a [publicly available map of districts](http://www.uscourts.gov/about-federal-courts/court-role-and-structure) that should give you an idea of how useful an interactive version could be.\n",
    "\n",
    "The main research question here are: \n",
    "1. How can an interactive map serve as a framework for investigating federal judicial nominations and approvals? \n",
    "2. How can we gather data and investigate the potential impact of these new judges?\n",
    "\n",
    "Your map may be divided in a few different ways:\n",
    "\n",
    "1. By the 13 districts\n",
    "2. By the regions of the 96 district courts\n",
    "3. By the physical locations of these courts (this is complicated, many districts do not have the single courthouse, but have a handful of them within the region)\n",
    "\n",
    " *You may choose to map one, two, or all of these geographical divisions--or you may come up with your own geographical divisions that you find more meaningful.\n",
    " \n",
    " *You may also choose to focus on a specific district or districts, if that is where your research takes you\n",
    "\n",
    "The information/reporting you make explorable through the map may focus on one or more of the following:\n",
    "\n",
    "1. Current judicial vacancies (which districts and courts have the most vacancies, the most recent appointees, what vacancies have taken the longest to fill, etc)\n",
    "2. Current members of each court (who are the current judges, how long have they been judges, what is the background of the judges--their ages, education, etc)\n",
    "3. Recent confirmations and confirmation hearings: by downloading and parsing the confirmation questionnaires and Q&A's (in PDF form) read for keywords for what is at stake during these hearings.\n",
    "\n",
    "\n",
    "The starting points for scraping include:\n",
    "\n",
    "Current judicial vacancies:\n",
    "http://www.uscourts.gov/judges-judgeships/judicial-vacancies/current-judicial-vacancies\n",
    "\n",
    "Confirmation listings:\n",
    "http://www.uscourts.gov/judges-judgeships/judicial-vacancies/confirmation-listing\n",
    "\n",
    "Archives of vacancies/confirmations (if you want to build more historical data)\n",
    "http://www.uscourts.gov/judges-judgeships/judicial-vacancies/archive-judicial-vacancies\n",
    "\n",
    "\n",
    "Hearings and PDFs of Q&A's for nominees:\n",
    "https://www.judiciary.senate.gov/meetings/05/23/2018/nominations\n",
    "\n",
    "Judiciary committee's list of confirmations including questionnaires and PDF:\n",
    "https://www.judiciary.senate.gov/nominations/confirmed\n",
    "\n",
    "\n",
    "Present and past judges including resumes:\n",
    "\n",
    "Appeals courts:\n",
    "https://www.fjc.gov/history/courts/u.s.-court-appeals-district-columbia-circuit-justices-and-judges\n",
    "\n",
    "District courts:\n",
    "https://www.fjc.gov/history/courts/u.s.-district-courts-and-federal-judiciary\n",
    "\n",
    "\n",
    "**Advantages of this option:** the main challenge with this option is the integration multiple data sources. In this option you are provided with many solid leads for primary sources\\*, and you could satisfy the requirements of this project simply by integrating and mapping everything. There is a lot of scraping of multiple pages, and potentially less use of regular expressions depending on how deep you go into the data. This is the most challenging option in terms of bringing your point of view to the data. How you proceed depends  on what kind of perspective you want bring to it. (Depending on the level of analysis you choose, you may complete this project, or it may serve as a good starting point for deeper exploration and analysis.)\n",
    "\n",
    "**A note on secondary sources:** In any of these projects you may find other resources (Wikipedia, etc.) that may have some of this information in scrape-friendly format. While those shortcuts might be appealing, see how much you can get from these primary resources. From a journalistic standpoint, Wikipedia should be the last resort--and do understand that their data frames reflect their choices of what's important. That said, for all of these projects, you may find outside sources in whatever format you wish, and use them to enhance your data--just be mindful of their reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Democratic candidate speech tracker\n",
    "This is a new and potentially very open (i.e. challenging) project. There are currently over 20 declared candidates for United States President in the 2020 Democratic Primary. Actually tracking and obtaining the campaign stops and speeches for each of these candidates is quite challenging. This recent New York Times article begins to map the trails for some of the candidates:\n",
    "\n",
    "https://www.nytimes.com/interactive/2019/04/05/us/politics/2020-democrats-campaign-stops.html\n",
    "\n",
    "It is a pretty nicely designed data visualization, but note that it is not interactive! One of the main advantages of an interactive map is to provide a framework for exploring the subject to more deeply, rather than just scratching the surface. \n",
    "\n",
    "The goal of this project is to scrape and/or gather information on campaign stops for some (or all) of the current democratic candidates. Mapping every democratic candidate will be impossible in this timeframe. The trick will be to choose two or more candidates and begin gathering information on campaign stops, the time the campaign was declared, and most importantly to locate the text of speeches at those campaign stops. \n",
    "\n",
    "There is no easy way to do this! There will be a lot of manual work involved. The most direct computational way I have found thus far is to search C-SPAN for televised speeches by the candidates, and scrape the closed-caption transcript of the speech. Here are some examples of links to speeches by different candidates:\n",
    "\n",
    "https://www.c-span.org/search/?sdate=&edate=&searchtype=Videos&sort=Most+Recent+Airing&text=0&all%5B%5D=speech&personid%5B%5D=1023023\n",
    "\n",
    "\n",
    "https://www.c-span.org/search/?sdate=&edate=&searchtype=Videos&sort=Most+Recent+Airing&text=0&all%5B%5D=speech&personid%5B%5D=1022862\n",
    "\n",
    "The C-SPAN method would entail individually locating the proper search results, scraping that search results page, manually weeding out any results that are not actually speeches, and then going to the individual pages--and using selenium to scrape the full text of the transcript. This is challenging, and there is a lot of manual work involved.\n",
    "\n",
    "If you can do this for two or three (even four!!??) candidates, that would be great!  You would then map the locations of the speeches and link each location to the text of the speech. (Note: Using the C-SPAN method means that you will be mapping C-SPAN's coverage of these candidates and you may only obtain a few speeches for some of the less popular candidates, but again, it's a start!)\n",
    "\n",
    "This is an open project, dealing with current news. The main goal is to build a database of campaign speeches by candidate including the location and date of the speech. You do not have to use the C-SPAN method. You can do manual research through the Internet and build this database by \"hand\", or seek out other possibilities for finding batches of speech transcipts online. Feel free to work in groups and to assign different candidates to different people in your group. If you find better resources, be sure to **share** them.\n",
    "\n",
    "Here are a few other links to resources about candidates, they may be helpful as a starting point:\n",
    "\n",
    "https://ballotpedia.org/Elizabeth_Warren_presidential_campaign,_2020\n",
    "\n",
    "https://www.politico.com/interactives/2019/04/25/2020-democratic-presidential-candidates-take-stands-issues/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 5: Presidential Speeches\n",
    "If you like the idea of option 4, but are concerned about the difficulty of the research and coding involved. This is a more simplified (and dated) version: mapping geographically and comparing the speeches of Barack Obama with the speeches of George W. Bush. Many of their speeches are available here:\n",
    "\n",
    "https://americanrhetoric.com/barackobamaspeeches.htm\n",
    "\n",
    "https://americanrhetoric.com/gwbushspeeches.htm\n",
    "\n",
    "You would scrape the links, download the PDFs, and use regular expressions to parse the PDFs and extract the text of the speeches. As you create this database think about what kind of questions you might ask about the differences, times and/or locations of these speeches. You would want to map each speech to the location of the speech: actual place, state or region. Or you might want to look for patterns, phrases or words that are common or distinct across these two presidents.\n",
    "\n",
    "*Note:* You may choose other presidents or world leaders from any country. If you choose to do this project, but with speeches by other presidents or world leaders, you would need to create a data set of up at least 20 speeches per person. Please submit the names of leaders you are choosing along with the data you have found so far and the potential research question you will be asking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps ahead:\n",
    "**A note on the map:** While the map is the main visual container and framework for this project, **it is the least important aspect of this project**. The main thing to understand about the map is that it defines the data architecture (in geojson format) for the final output of your data. The most important fields in your final geojson document are \"headline\" and \"article\"--that is where you will display the data that you have discovered, and conceptually that is the biggest challenge. As you move forward, you want to think about how you will develop this output. \n",
    "\n",
    "Just to give you a sense of the layout and information architecture of the map, here are two examples of the main templates. This will change a little bit, so do not attempt learn the actual code here!! Just look at how the information is distributed on the maps:\n",
    "\n",
    "Shapes: http://floatingmedia.com/columbia/map_shape/\n",
    "\n",
    "Points: http://floatingmedia.com/columbia/map_points/\n",
    "\n",
    "**Another note on the map:** you do not have to end up with a map. While it may be the path of least resistance, the important part is to develop the database and investigate the database. Many students have chosen to just present their findings in the format of bar charts or just summaries in their final presentations. Thinking geographically can limit your interpretation of this data and if you feel that it is not the most valid form of analysis it is perfectly legitimate to think in different ways.\n",
    "\n",
    "If you are proposing an independent project please submit your proposal by **Thursday at 4 PM.** And I will meet with you on Friday to discuss the possibility. If you're doing a guided project you need to choose your topic **by Monday**.\n",
    "\n",
    "-If you are deciding on one of these guided projects, look at the various links for the sources, and think about which level of subject matter and programmatic skill interests you the most. \n",
    "\n",
    "-As you consider each project, begin to think about an angle, a focus, a specific lens that you want to bring to the potential data set. Do some background reading so you are familiar with the topic.\n",
    "\n",
    "-For the guided projects you will be provided with a guided notebook that will get you started with the initial process of obtaining and transforming your data.\n",
    "\n",
    "-By next Friday you should have some of the data transformed, and a good idea of what kind of focus, level of analysis, and potential secondary source you will be using. \n",
    "\n",
    "-Note: These are not group projects--each student must engage in coding their own notebook--but you may work as groups, discussing conceptual approaches, assigning various levels of focus, collaborating on different forms of analysis. But each of you should do your own coding.\n",
    "\n",
    "-By the final week, your data will enter a pandas dataframe, which you will use to analyze, consolidate and finally export your data into geojson format. Next week's classes will be devoted primarily to pandas along with a brief intro into the mapping library (Remember the map design itself is only icing on the cake).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping downloading and converting many PDFs\n",
    "This is the cornerstone of the Supreme Court transcript project, but it may also be useful for the federal judge project as well--especially if you decide to download hearings material want to include Q&A text.\n",
    "\n",
    "I begin by scraping the links to all the transcriptions using **beautiful soup**--if you were to choose to do this project, you would also want to scrape this page for the rest of its information such as the name of the case, the docket number, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "my_url = \"https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx\"\n",
    "raw_html = requests.get(my_url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_doc = BeautifulSoup(raw_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_tables = soup_doc.find_all(class_=\"table table-bordered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pdf_links = []\n",
    "for table in the_tables:\n",
    "    good_row = table.find_all('tr')\n",
    "    for row in good_row:\n",
    "        if row.td is not None:\n",
    "            print(row.a['href'])\n",
    "            all_pdf_links.append(row.a['href'][3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pdf_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I used the **requests** library to download all of the PDFs to a folder on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for urls in all_pdf_links:\n",
    "    link = 'https://www.supremecourt.gov/oral_arguments/' + urls\n",
    "    book_name = \"/Users/Jon/Documents/Columbia2019/2018pdf/\" + link.split('/')[-1]\n",
    "    with open(book_name, 'wb') as book:\n",
    "        a = requests.get(link, stream=True)\n",
    "\n",
    "        for block in a.iter_content(512):\n",
    "            if not block:\n",
    "                break\n",
    "\n",
    "            book.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I make a list of the names of the PDFs\n",
    "pdf_names = [url.split('/')[-1] for url in all_pdf_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here I use the built in **os** library to run command line actions from Python. I am using the command line based **xpdf** tool (specifically its **pdftotext** command)that converts PDF to text in a way that's faster and simpler than using Python libraries that deal with PDFs. (This is certainly not the only way to do this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def pdf_to_text(name):\n",
    "    folder = \"/Users/Jon/Documents/Columbia2019/2018pdf/\"\n",
    "    input1 = folder + name\n",
    "    txt_name = name.replace(\".pdf\",\".txt\")\n",
    "    output1 = folder + txt_name\n",
    "    os.system(\"pdftotext '%s' '%s'\" % (input1, output1))\n",
    "\n",
    "#Here's an example of a single command    \n",
    "#os.system('pdftotext /Users/Jon/Documents/columbia_syllabus/pdf/16-605_2dp3.pdf /Users/Jon/Documents/columbia_syllabus/pdf/16-605_2dp3.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This those to the names and sends them to the function\n",
    "for pdf_file in pdf_names:\n",
    "    pdf_to_text(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/Jon/Documents/Columbia2019/2018pdf/17-5554_7648.txt', 'r')\n",
    "sample_transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
